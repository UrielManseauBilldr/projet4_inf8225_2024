{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BlDAqiDM03Wo"
      },
      "outputs": [],
      "source": [
        "%pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n",
        "!apt-get update > /dev/null 2>&1\n",
        "!apt-get install cmake > /dev/null 2>&1\n",
        "%pip install --upgrade setuptools 2>&1\n",
        "%pip install ez_setup > /dev/null 2>&1\n",
        "%pip install gym[classic-control] > /dev/null 2>&1\n",
        "%pip install gymnasium\n",
        "%pip install stable_baselines3\n",
        "%pip install shimmy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from stable_baselines3 import DDPG\n",
        "from stable_baselines3.common.noise import NormalActionNoise\n",
        "from stable_baselines3.common.evaluation import evaluate_policy\n",
        "from stable_baselines3.ddpg.policies import MlpPolicy\n",
        "from gym import logger as gymlogger\n",
        "from gym.wrappers.record_video import RecordVideo\n",
        "gymlogger.set_level(40) #error only\n",
        "import tensorflow as tf\n",
        "import random\n",
        "import math\n",
        "import glob\n",
        "import io\n",
        "import base64\n",
        "from IPython.display import HTML\n",
        "from IPython import display as ipythondisplay\n",
        "from stable_baselines3.common.callbacks import BaseCallback\n",
        "import statistics\n",
        "import itertools"
      ],
      "metadata": {
        "id": "-jSBWRxo06_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make(\"Pendulum-v1\",render_mode=\"rgb_array\")"
      ],
      "metadata": {
        "id": "5aOC39t-0_G0"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_actions = env.action_space.shape[-1]\n",
        "action_noise = NormalActionNoise(mean=np.zeros(n_actions), sigma=0.1 * np.ones(n_actions))"
      ],
      "metadata": {
        "id": "RxQgoWd71Aiu"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = DDPG(MlpPolicy, env, action_noise=action_noise, verbose=1)"
      ],
      "metadata": {
        "id": "uYUDS5Nf1Cok"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RewardLoggerCallback(BaseCallback):\n",
        "    def __init__(self, verbose=0):\n",
        "        super(RewardLoggerCallback, self).__init__(verbose)\n",
        "        self.rewards = []\n",
        "        self.cumulative_rewards = []\n",
        "\n",
        "    def _on_step(self) -> bool:\n",
        "        self.rewards.append(self.locals[\"rewards\"])\n",
        "        if self.locals[\"dones\"]:\n",
        "            self.cumulative_rewards.append(np.sum(self.rewards))\n",
        "            self.rewards = []\n",
        "        return True\n",
        "\n",
        "reward_logger = RewardLoggerCallback()"
      ],
      "metadata": {
        "id": "OcN-ZktE1H60"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the parameter grids\n",
        "learning_rates = [0.001, 0.01, 0.1]\n",
        "buffer_sizes = [1000, 10000, 100000]\n",
        "batch_sizes = [32, 64, 128]\n",
        "tau_values = [0.001, 0.01]\n",
        "gamma_values = [0.5, 0.99]\n",
        "\n",
        "# Create a DataFrame to store the results\n",
        "columns = ['configuration ID', 'learning_rate', 'buffer', 'batch', 'tau', 'gamma', 'Reward']\n",
        "results_df = pd.DataFrame(columns=columns)\n",
        "\n",
        "# Generate all combinations\n",
        "configuration_id = 0\n",
        "for learning_rate, buffer_size, batch_size, tau, gamma in itertools.product(\n",
        "    learning_rates, buffer_sizes, batch_sizes, tau_values, gamma_values):\n",
        "\n",
        "    # Initialize and train the model\n",
        "    model = DDPG(MlpPolicy, env, action_noise=action_noise, learning_rate=learning_rate, verbose=0, batch_size=batch_size, buffer_size=buffer_size, tau=tau, gamma=gamma)\n",
        "    model.learn(total_timesteps=10000)\n",
        "\n",
        "    # Evaluate the policy\n",
        "    rewards_list = evaluate_policy(model, model.get_env(), n_eval_episodes=20, return_episode_rewards=True)[0]\n",
        "\n",
        "    # Temporary DataFrame for this iteration\n",
        "    temp_df = pd.DataFrame({\n",
        "        'configuration ID': [configuration_id] * len(rewards_list),\n",
        "        'learning_rate': [learning_rate] * len(rewards_list),\n",
        "        'buffer': [buffer_size] * len(rewards_list),\n",
        "        'batch': [batch_size] * len(rewards_list),\n",
        "        'tau': [tau] * len(rewards_list),\n",
        "        'gamma': [gamma] * len(rewards_list),\n",
        "        'Reward': rewards_list\n",
        "    })\n",
        "\n",
        "    # Concatenate the temporary DataFrame with the main results DataFrame\n",
        "    results_df = pd.concat([results_df, temp_df], ignore_index=True)\n",
        "\n",
        "    # Increment configuration ID\n",
        "    configuration_id += 1\n",
        "\n",
        "# Show or save the DataFrame\n",
        "print(results_df)\n",
        "# Optionally, save to CSV\n",
        "results_df.to_csv('evaluation_results.csv', index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4tgMV7cAF4Vo",
        "outputId": "fa07704a-55b4-4310-bc54-8ed87bc9b992"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     configuration ID  learning_rate  buffer batch    tau  gamma       Reward\n",
            "0                   0          0.001    1000    32  0.001   0.50 -1352.338412\n",
            "1                   0          0.001    1000    32  0.001   0.50 -1381.263307\n",
            "2                   0          0.001    1000    32  0.001   0.50 -1383.176071\n",
            "3                   0          0.001    1000    32  0.001   0.50 -1494.011277\n",
            "4                   0          0.001    1000    32  0.001   0.50 -1502.690619\n",
            "...               ...            ...     ...   ...    ...    ...          ...\n",
            "2155              107          0.100  100000   128  0.010   0.99 -1081.014297\n",
            "2156              107          0.100  100000   128  0.010   0.99 -1672.557162\n",
            "2157              107          0.100  100000   128  0.010   0.99 -1608.784389\n",
            "2158              107          0.100  100000   128  0.010   0.99 -1631.250004\n",
            "2159              107          0.100  100000   128  0.010   0.99 -1391.510157\n",
            "\n",
            "[2160 rows x 7 columns]\n"
          ]
        }
      ]
    }
  ]
}